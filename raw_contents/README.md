# Introduction

이 문서는 Andrew Ng 교수의 Coursera class 인 Machine Learning 강의 에서 배운 내용을 정리한 문서이다. 강의 링크: https://www.coursera.org/learn/machine-learning

작성: 김지훈 (jihuun.k@gmail.com)

test


## Table Of Contents  
---


- [Machine Learning course 정리 by Andrew Ng](#machine-learning-course----by-andrew-ng)
- [1. Introduction](#1-introduction)
  * [1.1. Introduction](#11-introduction)
    + [1.1.1. 응용분야](#111-----)
    + [1.1.2. ML 두가지 정의](#112-ml-------)
    + [1.1.3. ML 알고리즘 종류](#113-ml--------)
    + [1.1.4. Supervised Learning (지도 학습)](#114-supervised-learning--------)
    + [1.1.5. Unsupervised Learning (비지도 학습)](#115-unsupervised-learning---------)
    + [1.1.6. 참고: Reinforcement Learning](#116-----reinforcement-learning)
  * [1.2. Linear Regression with One Variable](#12-linear-regression-with-one-variable)
    + [1.2.1. Model and Cost Function](#121-model-and-cost-function)
    + [1.2.2 Linear Regression](#122-linear-regression)
    + [1.2.3. Model Representation](#123-model-representation)
    + [1.2.4. Cost Function (비용함수)](#124-cost-function-------)
    + [1.2.5. Simplified Cost function of Linear Regression](#125-simplified-cost-function-of-linear-regression)
    + [1.2.6. Cost function of Linear Regression](#126-cost-function-of-linear-regression)
    + [1.2.7. Parameter Learning (Gradient Descent)](#127-parameter-learning--gradient-descent-)
    + [1.2.8. Gradient Descent 알고리즘 수학적 정의](#128-gradient-descent------------)
    + [1.2.9. Learning rate와 미분계수](#129-learning-rate------)
    + [1.2.10. Gradient Descent for Linear Regression](#1210-gradient-descent-for-linear-regression)
  * [1.3. Linear Algebra Review](#13-linear-algebra-review)
    + [1.3.1 Vector](#131-vector)
    + [1.3.2 Matrix addition and substriction and 스칼라곱](#132-matrix-addition-and-substriction-and-----)
    + [1.3.3. Matrix * Vector Multiplication](#133-matrix---vector-multiplication)
    + [1.3.4. Matrix * Matrix Multiplication](#134-matrix---matrix-multiplication)
    + [1.3.5. Matrix Multiplication Properties](#135-matrix-multiplication-properties)
    + [1.3.6. Inverse and Transpose](#136-inverse-and-transpose)
- [2. Linear Regression with Multiple Variables](#2-linear-regression-with-multiple-variables)
  * [2.1. Multivariate Linear Regression](#21-multivariate-linear-regression)
    + [2.1.1. Multiple Features](#211-multiple-features)
    + [2.1.2. Gradient Descent for Multiple Variables](#212-gradient-descent-for-multiple-variables)
    + [2.1.3. Gradient Descent in Practice I - Feature Scaling](#213-gradient-descent-in-practice-i---feature-scaling)
    + [2.1.4. Gradient Descent in Practice II - Learning Rate](#214-gradient-descent-in-practice-ii---learning-rate)
    + [2.1.5. Features and Polynomial Regression](#215-features-and-polynomial-regression)
  * [2.2. Computing Parameters Analytically](#22-computing-parameters-analytically)
    + [2.2.1. Normal Equation](#221-normal-equation)
    + [2.2.2. Normal Equation Noninvertibility](#222-normal-equation-noninvertibility)
- [3. Logistic Regression, Regularization](#3-logistic-regression--regularization)
  * [3.1. Classfication](#31-classfication)
  * [3.2. Logistic Regression](#32-logistic-regression)
  * [3.3. Decision Boundary](#33-decision-boundary)
  * [3.4. Non-linear Decision Boundary](#34-non-linear-decision-boundary)
  * [3.5. Cost function](#35-cost-function)
  * [3.6. Simplified Cost Function and Gradient Descent](#36-simplified-cost-function-and-gradient-descent)
  * [3.7. Advanced Optimization](#37-advanced-optimization)
  * [3.8. Multi-Class Classification](#38-multi-class-classification)
  * [3.9. Overfitting problem](#39-overfitting-problem)
    + [3.9.1. Overfitting 이란?](#391-overfitting----)
    + [3.9.2. Cost function](#392-cost-function)
    + [3.9.3. Regularized Linear Regression](#393-regularized-linear-regression)
    + [3.9.4. Regularized Logistic Regression](#394-regularized-logistic-regression)
  * [ex2-1: Logistic Regression](#ex2-1--logistic-regression)
    + [0. ex2.m](#0-ex2m)
    + [1. plotData.m](#1-plotdatam)
    + [2. sigmoid.m](#2-sigmoidm)
    + [3. costFuncion.m](#3-costfuncionm)
    + [4. predict.m](#4-predictm)
  * [ex2-2: Regularized Logistic Regression](#ex2-2--regularized-logistic-regression)
    + [1. mapFeature.m](#1-mapfeaturem)
    + [1. costFunctionReg.m](#1-costfunctionregm)
- [4. Neural Networks](#4-neural-networks)
  * [4.1. 배경](#41---)
  * [4.2. 지난 내용 remind](#42-------remind)
  * [4.3. Neuron model: Logistic Unit](#43-neuron-model--logistic-unit)
  * [4.4. Neural Network](#44-neural-network)
  * [4.5. Forward propagation: Vectored Implementation](#45-forward-propagation--vectored-implementation)
  * [4.6. Neural Network Learning its Own Features](#46-neural-network-learning-its-own-features)
  * [4.7. Examples](#47-examples)
    + [OR function](#or-function)
    + [The others](#the-others)
    + [Multiclass Classification](#multiclass-classification)
  * [ex3-1 Multi-class classification](#ex3-1-multi-class-classification)
    + [lrCostFunction.m](#lrcostfunctionm)
    + [oneVsAll.m](#onevsallm)
    + [predictOneVsAll.m](#predictonevsallm)
  * [ex3-2 Neural Networks](#ex3-2-neural-networks)
    + [predict.m](#predictm)
- [5. Neural Networks: Learning](#5-neural-networks--learning)
  * [5.1 Cost function](#51-cost-function)
  * [5.2. Backpropagation Algorithm](#52-backpropagation-algorithm)
  * [5.3. Backpropagation Intuition](#53-backpropagation-intuition)
  * [5.4. Backpropagation in Practice](#54-backpropagation-in-practice)
    + [5.4.1. A. Implementation Note: Unrolling Parameters](#541-a-implementation-note--unrolling-parameters)
    + [5.4.2. Gradient Checking](#542-gradient-checking)
    + [5.4.3. Random Initialization](#543-random-initialization)
    + [5.4.4. Putting it Together](#544-putting-it-together)
  * [ex4-1 : neural networks](#ex4-1---neural-networks)
    + [1. feedforward and cost function (without regularization)](#1-feedforward-and-cost-function--without-regularization-)
    + [2. regularized cost function](#2-regularized-cost-function)
  * [ex4-2 Backpropagation](#ex4-2-backpropagation)
    + [1. sigmoid gradient](#1-sigmoid-gradient)
    + [2. Random initialization](#2-random-initialization)
    + [3. Backpropagation](#3-backpropagation)
    + [4. Regularizaed Neural networks](#4-regularizaed-neural-networks)
  * [ex4.m](#ex4m)
- [6. Advice for Applying Machine Learning](#6-advice-for-applying-machine-learning)
  * [6.1. Evaluating a Hypothesis](#61-evaluating-a-hypothesis)
  * [6.2. Model Selection and Train/Validation/Test Sets](#62-model-selection-and-train-validation-test-sets)
  * [6.3.Diagnosing Bias vs. Variance](#63diagnosing-bias-vs-variance)
  * [6.4. Regularization and Bias/Variance](#64-regularization-and-bias-variance)
  * [6.5. Learning Curves](#65-learning-curves)
  * [6.6. Deciding What to Do Next Revisited](#66-deciding-what-to-do-next-revisited)
  * [ex5](#ex5)
    + [Part 2: Regularized Linear Regression Cost](#part-2--regularized-linear-regression-cost)
    + [Part 3: Regularized Linear Regression Gradient](#part-3--regularized-linear-regression-gradient)
    + [Part 4: Train Linear Regression](#part-4--train-linear-regression)
    + [Part 5: Learning Curve for Linear Regression](#part-5--learning-curve-for-linear-regression)
    + [Part 6: Feature Mapping for Polynomial Regression](#part-6--feature-mapping-for-polynomial-regression)
    + [Part 7: Learning Curve for Polynomial Regression](#part-7--learning-curve-for-polynomial-regression)
    + [Part 8: Validation for Selecting Lambda](#part-8--validation-for-selecting-lambda)
- [7. Machine Learning System Design](#7-machine-learning-system-design)
  * [7.1 Building a Spam Classifier](#71-building-a-spam-classifier)
  * [7.2. Handling Skewed Data](#72-handling-skewed-data)
  * [7.3. Using Large Data Sets](#73-using-large-data-sets)
- [8. Support Vector Machines](#8-support-vector-machines)
  * [8.1. Optimization Objective](#81-optimization-objective)
  * [8.2. Large margin classification](#82-large-margin-classification)
  * [8.3. Mathematics Behind Large Margin Classification](#83-mathematics-behind-large-margin-classification)
  * [8.4. Kernel](#84-kernel)
  * [8.5. Choosing the Landmarks](#85-choosing-the-landmarks)
  * [8.6. SVMs in Practice](#86-svms-in-practice)
  * [ex6-1. support vector machines](#ex6-1-support-vector-machines)
    + [1. Try different value of C](#1-try-different-value-of-c)
    + [2. Gaussian Kernel](#2-gaussian-kernel)
    + [3. Gaussian Kernel for non-linear](#3-gaussian-kernel-for-non-linear)
    + [4. Find best C and Sigma](#4-find-best-c-and-sigma)
  * [ex6-2 Spam classification](#ex6-2-spam-classification)
    + [Part 1: Email Preprocessing](#part-1--email-preprocessing)
    + [Part 2: Feature Extraction](#part-2--feature-extraction)
    + [Part 3: Train Linear SVM for Spam Classification](#part-3--train-linear-svm-for-spam-classification)
    + [Part 4: Test Spam Classification](#part-4--test-spam-classification)
    + [Part 5: Top Predictors of Spam](#part-5--top-predictors-of-spam)
    + [Part 6: Try Your Own Emails](#part-6--try-your-own-emails)
- [9. Unsupervised Learning, Dimensionality Reduction](#9-unsupervised-learning--dimensionality-reduction)
  * [9.1 K-Means Algorithm](#91-k-means-algorithm)
  * [9.2. Optimization Objective](#92-optimization-objective)
  * [9.3. Random Initialization](#93-random-initialization)
  * [9.4. Choosing the Number of Clusters](#94-choosing-the-number-of-clusters)
- [10. Dimensionality Reduction](#10-dimensionality-reduction)
  * [10.1. PCA (Principal Component Analysis)](#101-pca--principal-component-analysis-)
  * [10.2. PCA, Data pre-processing](#102-pca--data-pre-processing)
  * [10.3. PCA algorithm](#103-pca-algorithm)
  * [10.4. Reconstruction from Compressed Representation](#104-reconstruction-from-compressed-representation)
  * [10.5. Choosing the Number of Principal Components](#105-choosing-the-number-of-principal-components)
  * [10.6. Supervised Learning Speedup](#106-supervised-learning-speedup)
  * [ex7: K-means clustering and PCA](#ex7--k-means-clustering-and-pca)
    + [Part 1: Find Closest Centroids](#part-1--find-closest-centroids)
    + [Part 2: Compute Means](#part-2--compute-means)
- [11. Anormaly Detection](#11-anormaly-detection)
  * [11.1. Gaussian Distribution](#111-gaussian-distribution)
  * [11.2. Anormaly detection algorithm](#112-anormaly-detection-algorithm)
  * [11.3. Developing and Evaluating an Anomaly Detection System](#113-developing-and-evaluating-an-anomaly-detection-system)
  * [11.4. Anomaly Detection vs. Supervised Learning](#114-anomaly-detection-vs-supervised-learning)
  * [11.5. Choosing What Features to Use](#115-choosing-what-features-to-use)
  * [11.6. Multivariate Gaussian Distribution](#116-multivariate-gaussian-distribution)
- [12. Recommander Systems](#12-recommander-systems)
  * [12.1. Predicting Movie Ratings: Problem define](#121-predicting-movie-ratings--problem-define)
  * [12.2. Predicting Movie Ratings: Practice](#122-predicting-movie-ratings--practice)
  * [12.3. Collaborative Filtering](#123-collaborative-filtering)
  * [12.4. In practice of Collaborative Filtering](#124-in-practice-of-collaborative-filtering)
  * [12.5. Low Rank Matrix Factorization](#125-low-rank-matrix-factorization)
  * [12.6. Finding related movies](#126-finding-related-movies)
  * [12.7. Implementational Detail: Mean Normalization](#127-implementational-detail--mean-normalization)
  * [ex8: Recommander System](#ex8--recommander-system)
- [13. Large scale machine learning](#13-large-scale-machine-learning)
  * [13.1 Learning With Large Datasets](#131-learning-with-large-datasets)
  * [13.2. Stochastic Gradient Descent](#132-stochastic-gradient-descent)
  * [13.3. Mini-Batch Gradient Descent](#133-mini-batch-gradient-descent)
  * [13.4. Stochastic Gradient Descent Convergence](#134-stochastic-gradient-descent-convergence)
  * [13.5. Advanced Topics: Online Learning](#135-advanced-topics--online-learning)
  * [13.6. Advanced Topics: Map Reduce and Data Parallelism](#136-advanced-topics--map-reduce-and-data-parallelism)
- [14. Application Example: Photo OCR](#14-application-example--photo-ocr)
  * [14.1. Sliding Windows](#141-sliding-windows)
  * [14.2. Summary](#142-summary)
- [15. Vectorized implementations (in Octave)](#15-vectorized-implementations--in-octave-)
  * [15.1. Vector A가 있을때, $sum(A.^2)$ ?](#151-vector-a--------sum-a-2----)
  * [15.2. loop 제거방법](#152-loop-----)
  * [15.3. Built-in Function: vectorize (fun)](#153-built-in-function--vectorize--fun-)
  * [15.4. Vectorization built-in 함수 목록 in Octave](#154-vectorization-built-in-------in-octave)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>

